# Typeform Help Center RAG Chatbot

A production-ready Retrieval-Augmented Generation (RAG) chatbot system that answers questions about Typeform's Help Center using semantic search and large language models. 

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Features](#features)
- [Prerequisites](#prerequisites)
- [Quick Start](#quick-start)
- [Local Development](#local-development)
- [Docker Deployment](#docker-deployment)
- [Kubernetes Deployment](#kubernetes-deployment)
- [API Documentation](#api-documentation)
- [Configuration](#configuration)
- [Project Structure](#project-structure)
- [Technical Decisions](#technical-decisions)
- [Future Improvements](#future-improvements)
- [Troubleshooting](#troubleshooting)

## ğŸ¯ Overview

This chatbot implements a RAG (Retrieval-Augmented Generation) pipeline that:
1. Fetches and processes Typeform Help Center articles
2. Chunks content into semantically meaningful segments
3. Generates embeddings using OpenAI's text-embedding-3-small
4. Stores embeddings in Pinecone vector database
5. Retrieves relevant context for user queries
6. Generates accurate responses using GPT-4

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI REST API            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     RAG Engine               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚ Retrieval  â”‚Generation â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚           â”‚
             â–¼           â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Pinecone â”‚  â”‚ OpenAI â”‚
      â”‚  Vector  â”‚  â”‚  LLM   â”‚
      â”‚    DB    â”‚  â”‚        â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Breakdown

1. **Configuration** (`config.py`)
   - Centralized settings management with pydantic-settings
   - Environment variable validation and type safety
   - Manages API keys, models, and RAG parameters

2. **Sample Articles** (`sample_articles.py`)
   - Contains real Help Center article content
   - Two articles: multi-language forms and multi-question pages
   - Eliminates need for live web scraping per assignment requirements

3. **Data Models** (`data_ingestion.py`)
   - Article dataclass definition
   - Delegates to sample_articles for content loading
   - Maintains clean data structure for processing

4. **Document Processing** (`document_processor.py`)
   - Chunks text using RecursiveCharacterTextSplitter (1000 chars, 200 overlap)
   - Generates embeddings via OpenAI text-embedding-3-small
   - Maintains metadata throughout pipeline

5. **Vector Store** (`vector_store.py`)
   - Manages Pinecone index lifecycle
   - Handles batch vector upserts
   - Performs similarity search with cosine metric

6. **RAG Engine** (`rag_engine.py`)
   - Orchestrates retrieval and generation
   - Implements prompt engineering with system/user messages
   - Combines retrieved context with GPT-4 for responses

7. **API Layer** (`main.py`)
   - FastAPI REST endpoints with OpenAPI documentation
   - Request validation with Pydantic models
   - CORS configuration, admin token guard, request ID middleware
   - Error handling and structured logging

## âœ¨ Features

- âœ… **End-to-end RAG pipeline** from data ingestion to response generation
- âœ… **RESTful API** with FastAPI for easy integration
- âœ… **Semantic search** using Pinecone vector database
- âœ… **Production-ready** with Docker and Kubernetes support
- âœ… **Comprehensive logging** with structured logging
- âœ… **Error handling** at all pipeline stages
- âœ… **Scalable architecture** with horizontal pod autoscaling
- âœ… **Health checks** and readiness probes
- âœ… **Configuration management** via environment variables

## ğŸ“¦ Prerequisites

### Required for Local Development
- Python 3.11+ (Python 3.10+ supported, 3.11 recommended)
- [uv](https://docs.astral.sh/uv/) package manager (recommended) or pip
- OpenAI API key ([Get one here](https://platform.openai.com/api-keys))
- Pinecone API key ([Free tier available](https://www.pinecone.io/))

### Optional for Deployment
- Docker Desktop (for containerized deployment)
- Kubernetes cluster (for K8s deployment - not required to run locally)
- Helm 3+ (for Helm-based K8s deployment)

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
# Clone the repository
git clone <repository-url>
cd RAG_system

# Create virtual environment with uv (recommended)
uv venv --python python3.11
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies (includes dev tools for testing)
uv sync --dev

# For production-only (no test dependencies):
# uv sync

# Alternative: using pip
# python3 -m venv venv
# source venv/bin/activate
# pip install -r requirements.txt
```

### 2. Configure Environment

```bash
# Copy example environment file
cp .env.example .env

# Edit .env and add your API keys
nano .env
```

Required environment variables:
```env
OPENAI_API_KEY=sk-...
PINECONE_API_KEY=...
# PINECONE_ENVIRONMENT=gcp-starter  # Only needed for pod-based indexes (not serverless)
```

**Note:** All other settings have sensible defaults and don't need to be changed.

### 3. Initialize Vector Database

```bash
# This script loads articles, chunks them, generates embeddings, and uploads to Pinecone
uv run python init_db.py

# Or if using regular Python:
# python init_db.py
```

Expected output:
```
============================================================
Typeform Help Center - Vector DB Initialization
============================================================
Step 1: Initializing Pinecone vector store
âœ“ Vector store initialized

Step 2: Loading Help Center articles
âœ“ Loaded 2 articles
  1. Create multi-language forms
  2. Add a Multi-Question Page to your form

Step 3: Processing articles (chunking and embedding)
âœ“ Created X chunks with embeddings

Step 4: Uploading to Pinecone
âœ“ Uploaded X vectors in Y batches

Step 5: Verifying index
âœ“ Index contains X vectors
  Dimension: 1536
  Fullness: Z%

============================================================
âœ“ Initialization complete! You can now start the API server.
============================================================
```

### 4. Start the API Server

```bash
# Using uv (recommended)
uv run python -m uvicorn main:app --host 0.0.0.0 --port 8000

# Or simpler:
uv run python main.py

# Or if using regular Python:
# python main.py
```

The API will be available at `http://localhost:8000`

**Browse the API docs:** `http://localhost:8000/docs`

### 5. Test the API

```bash
# Check health
curl http://localhost:8000/health

# Ask a question
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "How do I create a multi-language form?",
    "top_k": 3,
    "include_sources": true
  }'
```

## ğŸ’» Local Development

### Development Mode

```bash
# Install development dependencies (already included with uv sync --dev)
# Or with pip:
# pip install pytest pytest-asyncio httpx

# Run with auto-reload
uv run uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### API Documentation

Once running, visit:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

### Running Tests

```bash
uv run pytest

# Or if using regular Python:
# pytest
```

## ğŸ³ Docker Deployment

### Build and Run with Docker

```bash
# Build the image
docker build -t typeform-chatbot:latest .

# Run the container
docker run -p 8000:8000 \
  --env-file .env \
  typeform-chatbot:latest
```

### Using Docker Compose (Recommended)

```bash
# Start services
docker-compose up -d

# View logs
docker-compose logs -f

# Check status
docker-compose ps

# Restart services
docker-compose restart

# Stop services
docker-compose down

# Rebuild and start (after code changes)
docker-compose up -d --build
```

### Initialize Database in Docker

```bash
# Run initialization script in container
docker-compose run typeform-chatbot uv run python init_db.py

# Or with regular Python:
# docker-compose run typeform-chatbot python init_db.py
```

## â˜¸ï¸ Kubernetes Deployment

### Option 1: Plain Kubernetes Manifests

```bash
# Create namespace (optional)
kubectl create namespace chatbot

# Apply configurations
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
kubectl apply -f k8s/hpa.yaml

# Optional: Apply Ingress
kubectl apply -f k8s/ingress.yaml

# Check deployment status
kubectl get pods
kubectl get svc
```

**Important**: Update the secrets in `k8s/configmap.yaml` with your API keys before deploying.

### Option 2: Helm Deployment (Recommended)

```bash
# First, build and push the Docker image to your registry
docker build -t your-registry/typeform-chatbot:latest .
docker push your-registry/typeform-chatbot:latest

# Install the chart
helm install typeform-chatbot ./helm/typeform-chatbot \
  --set secrets.openaiApiKey="sk-..." \
  --set secrets.pineconeApiKey="..." \
  --set image.repository="your-registry/typeform-chatbot" \
  --set image.tag="latest"

# Or using a values file (edit helm/typeform-chatbot/values.yaml first)
helm install typeform-chatbot ./helm/typeform-chatbot \
  -f helm/typeform-chatbot/values.yaml

# Check status
helm status typeform-chatbot
helm list

# View pods
kubectl get pods -l app.kubernetes.io/name=typeform-chatbot

# Upgrade deployment
helm upgrade typeform-chatbot ./helm/typeform-chatbot

# Uninstall
helm uninstall typeform-chatbot
```

### Initialize Database in Kubernetes

```bash
# Create a one-time job to initialize the database
kubectl run init-db --rm -it --restart=Never \
  --image=typeform-chatbot:latest \
  --env="OPENAI_API_KEY=$OPENAI_API_KEY" \
  --env="PINECONE_API_KEY=$PINECONE_API_KEY" \
  -- uv run python init_db.py

# Or with regular Python:
# kubectl run init-db --rm -it --restart=Never \
#   --image=typeform-chatbot:latest \
#   --env="OPENAI_API_KEY=$OPENAI_API_KEY" \
#   --env="PINECONE_API_KEY=$PINECONE_API_KEY" \
#   -- python init_db.py
```

**Note:** The image must be pushed to a container registry accessible by your Kubernetes cluster.

### Accessing the Service

```bash
# If using LoadBalancer
kubectl get svc typeform-chatbot-service
# Use the EXTERNAL-IP

# If using port-forward
kubectl port-forward svc/typeform-chatbot-service 8000:80
# Access at localhost:8000
```

## ğŸ“š API Documentation

### Endpoints

#### `GET /`
Root endpoint with API information.

#### `GET /health`
Health check endpoint. Returns basic service status without verifying external dependencies.

**Response:**
```json
{
  "status": "healthy",
  "environment": "development",
  "vector_store_initialized": true,
  "rag_engine_initialized": true
}
```

#### `GET /ready`
Readiness probe endpoint. Verifies service initialization and Pinecone connectivity. Used by Kubernetes readiness probes.

**Response (Success):**
```json
{
  "status": "ready",
  "environment": "development",
  "vector_store_initialized": true,
  "rag_engine_initialized": true
}
```

**Response (Service Unavailable - 503):**
```json
{
  "detail": "Services not initialized"
}
```
or
```json
{
  "detail": "Vector store not ready"
}
```

#### `POST /query`
Query the chatbot with a question. Performs the complete RAG pipeline: embeds the question, retrieves relevant context, and generates an answer using GPT-4.

**Request Parameters:**
- `question` (required, string): User question, 1-1000 characters
- `top_k` (optional, integer): Number of document chunks to retrieve, 1-10, defaults to 3
- `include_sources` (optional, boolean): Include source URLs in response, defaults to true

**Request Example:**
```json
{
  "question": "How do I create a multi-language form?",
  "top_k": 3,
  "include_sources": true
}
```

**Response (Success - 200):**
```json
{
  "answer": "To create a multi-language form in Typeform, you can use the multi-language feature...",
  "query": "How do I create a multi-language form?",
  "num_sources": 2,
  "sources": [
    {
      "title": "Create multi-language forms",
      "url": "https://www.typeform.com/help/a/create-multi-language-forms-4405780/",
      "relevance_score": 0.89
    }
  ]
}
```

**Response (Service Unavailable - 503):**
```json
{
  "detail": "RAG engine not initialized"
}
```

**Response (Internal Server Error - 500):**
```json
{
  "detail": "Query processing failed: [error details]"
}
```

#### `POST /index/populate`
Populate the vector store with Help Center articles. Loads articles, chunks content, generates embeddings, and uploads to Pinecone.

**Authentication:** Requires `X-Admin-Token` header if `ADMIN_TOKEN` is configured.

**Request Headers:**
```
X-Admin-Token: your-admin-token-here
```

**Response (Created - 201):**
```json
{
  "message": "Index populated successfully",
  "articles_processed": 2,
  "chunks_created": 21,
  "total_upserted": 21,
  "batches": 1
}
```

**Response (Unauthorized - 401):**
```json
{
  "detail": "Invalid admin token"
}
```

**Response (Service Unavailable - 503):**
```json
{
  "detail": "Vector store not initialized"
}
```

**Response (Not Found - 404):**
```json
{
  "detail": "No articles loaded"
}
```

#### `GET /index/stats`
Get Pinecone index statistics including vector count, dimension, and storage utilization.

**Response (Success - 200):**
```json
{
  "total_vectors": 21,
  "dimension": 1536,
  "index_fullness": 0.0001
}
```

**Response (Service Unavailable - 503):**
```json
{
  "detail": "Vector store not initialized"
}
```

**Response (Internal Server Error - 500):**
```json
{
  "detail": "Failed to retrieve index stats: [error details]"
}
```

#### `DELETE /index/clear`
Delete all vectors from the Pinecone index. **Use with caution** - this operation cannot be undone.

**Authentication:** Requires `X-Admin-Token` header if `ADMIN_TOKEN` is configured.

**Request Headers:**
```
X-Admin-Token: your-admin-token-here
```

**Response (Success - 200):**
```json
{
  "message": "Index cleared successfully",
  "vectors_deleted": 21
}
```

**Response (Unauthorized - 401):**
```json
{
  "detail": "Invalid admin token"
}
```

**Response (Service Unavailable - 503):**
```json
{
  "detail": "Vector store not initialized"
}
```

**Response (Internal Server Error - 500):**
```json
{
  "detail": "Failed to clear index: [error details]"
}
```

### Important Notes

- **Swagger UI Issue**: When testing in the `/docs` UI, the `top_k` parameter incorrectly defaults to 0 instead of using the configured `TOP_K_RESULTS` default (3). You must manually set it to a value between 1-10 before submitting, or omit it entirely to use the system default. When using `curl` or other API clients, you can omit `top_k` and it will automatically use the configured default.
- **Request IDs**: All responses include an `X-Request-ID` header for request tracing and log correlation.
- **CORS**: CORS is enabled for all origins by default. Configure `ALLOWED_ORIGINS` in `.env` to restrict access.
- **Rate Limiting**: No rate limiting is currently implemented. Consider adding this for production deployments.

## âš™ï¸ Configuration

All configuration is managed via environment variables. See `.env.example` for all available options.

### Configuration Variables

#### Required Settings

| Variable | Description |
|----------|-------------|
| `OPENAI_API_KEY` | OpenAI API key for embeddings and LLM ([Get one here](https://platform.openai.com/api-keys)) |
| `PINECONE_API_KEY` | Pinecone API key for vector database ([Free tier available](https://www.pinecone.io/)) |

#### Pinecone Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `PINECONE_ENVIRONMENT` | (optional) | Legacy parameter for pod-based indexes. **Not needed for serverless indexes** (used in this project) |
| `PINECONE_INDEX_NAME` | typeform-help-center | Name of the Pinecone index to use |

#### Application Settings

| Variable | Default | Description |
|----------|---------|-------------|
| `APP_ENV` | development | Application environment: `development`, `staging`, or `production` |
| `LOG_LEVEL` | INFO | Logging verbosity: `DEBUG`, `INFO`, `WARNING`, or `ERROR` |
| `API_PORT` | 8000 | Port for the API server |
| `API_HOST` | 0.0.0.0 | Host address for the API server |
| `ALLOWED_ORIGINS` | * | CORS allowed origins (comma-separated, or `*` for all) |
| `ADMIN_TOKEN` | (empty) | Optional token for protecting admin endpoints (`/index/populate`, `/index/clear`) |

#### RAG Pipeline Settings

| Variable | Default | Description |
|----------|---------|-------------|
| `EMBEDDING_MODEL` | text-embedding-3-small | OpenAI embedding model (alternatives: text-embedding-3-large) |
| `LLM_MODEL` | gpt-4-turbo-preview | OpenAI LLM model (alternatives: gpt-3.5-turbo, gpt-4) |
| `CHUNK_SIZE` | 1000 | Maximum characters per text chunk |
| `CHUNK_OVERLAP` | 200 | Character overlap between consecutive chunks |
| `TOP_K_RESULTS` | 3 | Default number of chunks to retrieve (used when request doesn't specify `top_k`) |
| `MAX_TOKENS` | 500 | Maximum tokens in LLM response |
| `TEMPERATURE` | 0.7 | LLM temperature (0.0-2.0, lower = more deterministic) |

## ğŸ“ Project Structure

```
RAG_system/
# Core Application
â”œâ”€â”€ main.py                  # FastAPI application & API endpoints
â”œâ”€â”€ config.py                # Configuration management (pydantic-settings)
â”œâ”€â”€ rag_engine.py            # RAG pipeline orchestration
â”œâ”€â”€ vector_store.py          # Pinecone vector database integration
â”œâ”€â”€ document_processor.py    # Text chunking and embedding generation
â”œâ”€â”€ data_ingestion.py        # Article data models and loading
â”œâ”€â”€ sample_articles.py       # Pre-fetched Help Center article content
â”œâ”€â”€ exceptions.py            # Custom exception classes
â”œâ”€â”€ init_db.py               # Database initialization script

# Testing
â”œâ”€â”€ test_main.py             # API integration tests with stubbed dependencies

# Configuration & Dependencies
â”œâ”€â”€ .env.example             # Example environment variables
â”œâ”€â”€ pyproject.toml           # Python project metadata & dependencies (uv)
â”œâ”€â”€ requirements.txt         # Python dependencies (pip/Docker)
â”œâ”€â”€ uv.lock                  # Locked dependency versions (uv)

# Docker
â”œâ”€â”€ Dockerfile               # Multi-stage Docker image definition
â”œâ”€â”€ docker-compose.yml       # Docker Compose service configuration
â”œâ”€â”€ .dockerignore            # Docker build exclusions

# Version Control
â”œâ”€â”€ .gitignore               # Git ignore patterns

# Kubernetes Manifests
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ configmap.yaml       # Environment configuration
â”‚   â”œâ”€â”€ deployment.yaml      # Deployment with health probes
â”‚   â”œâ”€â”€ service.yaml         # LoadBalancer service
â”‚   â”œâ”€â”€ ingress.yaml         # Optional ingress rules
â”‚   â””â”€â”€ hpa.yaml             # Horizontal Pod Autoscaler

# Helm Chart
â””â”€â”€ helm/
    â””â”€â”€ typeform-chatbot/
        â”œâ”€â”€ Chart.yaml       # Chart metadata
        â”œâ”€â”€ values.yaml      # Default configuration values
        â””â”€â”€ templates/
            â”œâ”€â”€ deployment.yaml
            â”œâ”€â”€ service.yaml
            â”œâ”€â”€ configmap.yaml
            â”œâ”€â”€ secret.yaml
            â”œâ”€â”€ hpa.yaml
            â”œâ”€â”€ ingress.yaml
            â””â”€â”€ _helpers.tpl # Template helpers
```

## ğŸ¯ Technical Decisions

### 1. **Framework Selection: FastAPI**
**Why?**
- Modern, fast, and production-ready
- Automatic API documentation (Swagger/ReDoc)
- Built-in request validation with Pydantic
- Async support for better performance
- Type hints for better code quality

### 2. **Vector Database: Pinecone**
**Why?**
- Fully managed (no infrastructure overhead)
- Free tier suitable for this project
- Excellent performance for semantic search
- Simple API
- Production-ready with high availability

**Alternatives Considered:**
- Weaviate: More complex setup
- Qdrant: Good but requires self-hosting
- ChromaDB: Better for local development

### 3. **Embedding Model: text-embedding-3-small**
**Why?**
- **Cost-effective**: $0.02/1M tokens (62x cheaper than text-embedding-3-large)
- **1536 dimensions**: Optimal balance between quality and performance
  - Large enough to capture semantic nuance
  - Small enough for fast similarity search
  - Compatible with most vector databases' free tiers
- **High quality**: MTEB benchmark score of 62.3% (comparable to much larger models)
- **Fast inference**: ~10ms latency for embedding generation

**Alternatives Considered:**
- text-embedding-3-large (3072 dims): Better quality but 15x more expensive, slower search
- text-embedding-ada-002 (1536 dims): Older model, lower quality
- Open-source models (e.g., sentence-transformers): Requires self-hosting infrastructure

### 4. **LLM: GPT-4-turbo-preview**
**Why?**
- **High-quality responses**: Superior accuracy for help documentation (fewer hallucinations)
- **Large context window**: 128k tokens allows including multiple article chunks without truncation
- **Better reasoning**: Handles complex, multi-part questions more effectively than GPT-3.5
- **Cost-benefit**: For low-volume help center queries, quality outweighs the ~10x cost increase over GPT-3.5
- **Instruction following**: More reliable at staying within system prompt constraints

**For Production at Scale:**
- **High-volume, simple queries**: GPT-3.5-turbo ($0.50/1M tokens vs $10/1M)
- **Latency-critical**: GPT-3.5-turbo has ~2x faster response times
- **Custom domain**: Fine-tune GPT-3.5 on Typeform-specific Q&A pairs
- **Cost optimization**: Route queries by complexity (simple â†’ GPT-3.5, complex â†’ GPT-4)

### 5. **Data Source: Pre-fetched Articles**
**Approach:**
- Articles stored directly in `sample_articles.py`
- Content manually extracted from assignment URLs
- No live web scraping required

**Why?**
- **Assignment guidance**: "We do not require you to actively crawl the provided URLs"
- **Reliability**: No dependency on Help Center availability during evaluation
- **Simplicity**: Avoids scraping complexity (anti-bot measures, rate limits, HTML parsing)
- **Reproducibility**: Ensures consistent results across runs
- **Fast initialization**: No network delays during database setup

**For Production:**
- Implement scheduled scraping with change detection
- Add content versioning to track article updates
- Use headless browser (Playwright/Selenium) for JavaScript-heavy pages
- Respect robots.txt and implement rate limiting

### 6. **Chunking Strategy**
**Approach:**
- RecursiveCharacterTextSplitter with hierarchical separators
- 1000 characters per chunk with 200 character overlap
- Splits on: paragraph â†’ sentence â†’ word boundaries

**Why These Parameters?**
- **1000 chars**: Captures complete thoughts without overwhelming context window
  - Too small (< 500): Fragments ideas, loses context
  - Too large (> 2000): Dilutes relevance, increases noise
- **200 char overlap**: Prevents losing context at chunk boundaries
  - Ensures questions spanning boundaries still match relevant chunks
- **Hierarchical splitting**: Preserves semantic coherence by respecting natural language structure

**Alternatives Considered:**
- **Sentence-based splitting**: Too rigid, creates highly variable chunk sizes
- **Fixed-size chunks**: Breaks mid-sentence, poor readability
- **Semantic chunking** (LLM-based): Better quality but 10x slower and more expensive

**Empirical Testing:**
- Tested chunk sizes: 500, 750, 1000, 1500, 2000 characters
- 1000 chars provided best balance of relevance precision and context coverage

### 7. **LangChain Integration: Selective Usage**
**Approach:**
- Use LangChain for specific utilities (TextSplitter, embeddings wrapper, ChatOpenAI)
- Avoid heavy framework dependencies (chains, agents, memory)
- Direct OpenAI SDK usage where possible

**Why Selective?**
- **Utility value**: Text splitting and embeddings abstractions are well-designed
- **Avoid lock-in**: Framework-heavy approaches make provider switching harder
- **Transparency**: Direct control over prompts, retrieval, and generation logic
- **Simplicity**: Reduces abstraction layers, easier debugging

**Trade-offs:**
- **Missing features**: No built-in conversation memory, prompt templates, or chain orchestration
- **More code**: Need to implement RAG orchestration manually

**For Production:**
- Consider LangChain Expression Language (LCEL) for complex multi-step pipelines
- Use LangSmith for production monitoring and prompt versioning

### 8. **Package Manager: uv**
**Why uv over pip?**
- **Speed**: 10-100x faster dependency resolution and installation
- **Deterministic**: Lock file (`uv.lock`) ensures reproducible builds
- **Modern workflow**: pyproject.toml as single source of truth (PEP 621)
- **Better dependency management**: Separates production and dev dependencies cleanly
- **Active development**: Built by Astral (same team as Ruff), well-maintained

**Trade-offs:**
- **Dual requirements**: Must maintain both `pyproject.toml` (uv) and `requirements.txt` (Docker/pip)
- **Learning curve**: Less familiar than pip for some developers
- **Tooling maturity**: Newer than pip, some edge cases still being discovered

**Why keep requirements.txt?**
- Docker ecosystem still pip-centric
- Broader compatibility for users without uv installed
- CI/CD systems may not have uv pre-installed

### 9. **Testing Strategy: Stubbed Integration Tests**
**Approach:**
- Integration-style tests using FastAPI TestClient
- Stubbed external dependencies (StubRAG, StubVectorStore)
- No actual API calls to OpenAI or Pinecone during tests

**Why?**
- **Speed**: Tests run in <1 second without network calls
- **Reliability**: No flaky tests due to API rate limits or outages
- **Cost**: Avoid burning API credits on every test run
- **CI-friendly**: Works offline, no secrets management in CI
- **Focus**: Tests API contract and request/response handling, not external services

**Trade-offs:**
- **Limited coverage**: Doesn't catch integration issues with real APIs
- **Stub maintenance**: Stubs must stay synchronized with real implementations

**For Production:**
- Add separate integration tests against real APIs (marked with `@pytest.mark.integration`)
- Add contract tests to verify API responses match stub behavior
- Implement smoke tests in staging environment

### 10. **Similarity Metric: Cosine Similarity**
**Why Cosine over Alternatives?**
- **Normalized by default**: text-embedding-3-small produces unit vectors, making cosine optimal
- **Scale-invariant**: Measures angle between vectors, not magnitude
- **Industry standard**: Most RAG systems use cosine for text embeddings
- **Pinecone default**: Best supported in Pinecone's serverless architecture

**Alternatives:**
- **Dot product**: Faster but assumes normalized vectors (equivalent to cosine for OpenAI embeddings)
- **Euclidean distance**: Less meaningful for high-dimensional sparse vectors

### 11. **Containerization: Docker**
**Why?**
- **Environment consistency**: Eliminates "works on my machine" issues
- **Reproducible builds**: Lock Python version, system dependencies, and app code
- **Easy deployment**: Single artifact deployable anywhere (AWS, GCP, local)
- **Industry standard**: Expected for production deployments
- **Kubernetes compatibility**: Required for K8s deployments

**Docker Implementation:**
- **Multi-stage build**: Separate build and runtime stages to minimize image size
- **Non-root user**: Security best practice (runs as `appuser`, not `root`)
- **Health checks**: Container self-monitoring with `/health` endpoint
- **Layer caching**: Dependencies installed before code copy for faster rebuilds

### 12. **Orchestration: Kubernetes + Helm**
**Why?**
- **Production-grade orchestration**: Self-healing, automated rollouts/rollbacks
- **Auto-scaling**: HorizontalPodAutoscaler adjusts replicas based on CPU/memory
- **Configuration management**: Helm templating for multi-environment deployments
- **Industry standard**: Expected for production cloud deployments

**Key K8s Features Used:**
- **HorizontalPodAutoscaler**: Scale based on load (2-10 replicas)
- **Health/Readiness Probes**: Ensure traffic only routes to healthy pods
- **ConfigMaps/Secrets**: Separate config from code, secure API keys
- **LoadBalancer Service**: External access with cloud provider integration

### 13. **Logging: Structlog**
**Why?**
- Structured JSON logs
- Better for production monitoring
- Easy integration with log aggregation tools
- Context preservation across pipeline

### 14. **Project Structure: Flat Module Layout**
**Current Approach:**
- Flat structure with all Python modules at root level
- Direct imports without package hierarchy
- Simple, straightforward organization

**Why This Works for This Project:**
- **Scope-appropriate**: ~10 modules don't justify complex package hierarchies
- **Clarity**: Reviewers can quickly navigate and understand the codebase
- **Rapid development**: Flat structure enables fast iteration for MVP/proof-of-concept
- **Maintainable**: Clear separation of concerns (API, services, data) even without nested folders
- **Reduced complexity**: No import path confusion or circular dependency risks for small codebase

**When to Evolve:**
For a production system at scale, I would reorganize into a proper package structure:
```
src/
â”œâ”€â”€ api/          # FastAPI routes and dependencies
â”œâ”€â”€ core/         # Config, exceptions, logging
â”œâ”€â”€ services/     # RAG engine, vector store, processors
â”œâ”€â”€ models/       # Pydantic schemas
â””â”€â”€ data/         # Data loading and ingestion
tests/
â”œâ”€â”€ unit/
â””â”€â”€ integration/
```

This would provide:
- Better namespace management
- Clearer import paths (`from src.services.rag_engine import RAGEngine`)
- Separation of test types
- Integration into larger codebases without conflicts

**Trade-off Rationale:** For a demonstration project, simplicity and readability take precedence over enterprise-level organization. The current structure allows reviewers to quickly assess code quality, RAG understanding, and technical implementation without navigating deep folder hierarchies.

## ğŸš€ Future Improvements

Given more time, here are improvements I would implement, prioritized by impact:

### 1. **RAG Quality Enhancements** (High Priority)
- **Hybrid Search**: Combine dense (vector) + sparse (BM25) retrieval for better recall
  - Addresses cases where keyword matching outperforms semantic search
  - Implementation: Add Elasticsearch alongside Pinecone, merge results with Reciprocal Rank Fusion
- **Reranking**: Cross-encoder model (e.g., Cohere Rerank, bge-reranker) after initial retrieval
  - Re-scores top-k chunks for better precision
  - Expected improvement: 10-20% increase in answer accuracy
- **Query Enhancement**:
  - Hypothetical Document Embeddings (HyDE): Generate hypothetical answer, embed it, search with that
  - Query expansion: Use LLM to generate query variations before retrieval
  - Query decomposition: Break complex questions into sub-queries
- **Answer Validation**: Cross-check generated answers against retrieved chunks to detect hallucinations
- **Few-shot Examples**: Add 3-5 domain-specific examples in system prompt to improve output format consistency

### 2. **Architecture & Code Quality** (High Priority)
- **Package Restructuring**: Reorganize into `src/` structure (api, core, services, models, data)
  - Enables better testing, namespace management, and integration into larger systems
- **Dependency Injection**: Use dependency injection pattern for easier testing and modularity
- **Service Layer**: Separate business logic from API routes for better testability
- **Interface Abstractions**: Create protocols/ABCs for VectorStore, LLM, Embeddings to enable easy provider swapping
- **Unit Test Coverage**: Achieve >80% coverage with focused unit tests for each module
- **Documentation**: Add docstrings (Google style), type hints, and API reference docs

### 3. **Performance & Scalability** (Medium Priority)
- **Caching Layer**:
  - Redis cache for query results (TTL: 1 hour)
  - Cache embeddings for frequently accessed chunks
  - Expected: 10x faster response for repeated queries, 90% cost reduction
- **Async Pipeline**: Convert to fully async (embeddings, vector search, LLM calls in parallel)
  - Expected: 30-40% latency reduction for multi-chunk retrieval
- **Database Connection Pooling**: Reuse Pinecone connections across requests
- **Request Batching**: Batch multiple embeddings/LLM calls when handling concurrent requests
- **Streaming Responses**: Stream LLM tokens as they're generated (better perceived performance)
- **CDN for Static Assets**: If adding a web UI, serve from CDN

### 4. **Observability & Monitoring** (High Priority for Production)
- **Metrics (Prometheus)**:
  - Request latency (p50, p95, p99)
  - Token usage and costs per request
  - Cache hit rates
  - Error rates by type (API errors, timeouts, validation failures)
- **Distributed Tracing (OpenTelemetry)**: Trace requests through embedding â†’ retrieval â†’ generation
- **Logging Enhancements**:
  - Log query intents and retrieved chunk relevance scores
  - Sample queries for quality monitoring
  - Structured error logs with stack traces
- **Dashboards (Grafana)**:
  - Real-time metrics visualization
  - Cost tracking dashboard
  - Query success/failure trends
- **Alerting**: PagerDuty/Slack alerts for error rate spikes, high latency, API quota limits

### 5. **Security & Reliability** (Critical for Production)
- **Authentication & Authorization**:
  - JWT-based authentication for API access
  - Role-based access control (RBAC) for admin endpoints
  - API key rotation mechanism
- **Rate Limiting**: Per-user/IP rate limits (e.g., 100 req/min) using token bucket algorithm
- **Input Validation**:
  - Sanitize inputs to prevent injection attacks
  - Length limits, regex validation for special characters
  - Content filtering for inappropriate queries
- **Secrets Management**: Migrate from .env to Vault or AWS Secrets Manager
- **HTTPS/TLS**: Enforce encrypted connections in production
- **Data Privacy**: PII detection and redaction in logs
- **DDoS Protection**: Cloudflare or AWS Shield for traffic filtering

### 6. **Data Pipeline & Content Management** (Medium Priority)
- **CMS/Database Integration** (Production Reality):
  - Direct integration with internal CMS (Contentful, Strapi) or content database
  - Internal API consumption instead of web scraping
  - Eliminates scraping fragility, HTML parsing complexity
- **Real-time Sync**:
  - Change Data Capture (CDC) from source database (Debezium, AWS DMS)
  - Event-driven updates via message queue (Kafka, RabbitMQ, SQS)
  - Webhook listeners for CMS publish events
  - Expected latency: Content live in RAG within seconds of publish
- **Incremental Indexing**: 
  - Hash-based content diffing to only re-index modified chunks
  - Upsert strategy: Update changed articles, delete removed articles
  - Reduces compute cost by 90% vs full re-indexing
- **Version Control**: 
  - Track article versions in vector metadata (version_id, published_at)
  - Enable point-in-time queries ("What did docs say in Q4 2025?")
  - Rollback capability for bad content updates
- **Multi-source Support**: 
  - Ingest from multiple internal sources:
    - Help Center articles (primary)
    - Internal documentation (Confluence, Notion)
    - Support ticket resolutions (common solutions)
    - Community forum posts (user-generated insights)
  - Namespace isolation per source in Pinecone
- **Metadata Enrichment**:
  - Article metadata: categories, tags, author, last-updated, publish status
  - Analytics data: view counts, helpfulness ratings, search frequency
  - Enable hybrid filtering: "Only show articles from 'Forms' category with >100 views"
- **Content Quality Pipeline**:
  - Pre-indexing validation: broken links, formatting issues, duplicate detection
  - Automated quality scoring based on completeness, readability
  - Flag low-quality content for human review before indexing

### 7. **User Experience & Feedback** (Medium Priority)
- **Conversation Memory**: Multi-turn conversations with context retention (session-based chat history)
- **Confidence Scoring**: Return confidence scores with answers (based on chunk relevance + LLM uncertainty)
- **Suggested Follow-ups**: Generate related questions based on current query
- **Feedback Collection**:
  - Thumbs up/down on answers
  - "Was this helpful?" with optional comments
  - Store feedback for model improvement
- **Source Highlighting**: Show which parts of retrieved chunks were most relevant
- **Interactive Clarification**: Ask disambiguating questions for ambiguous queries

### 8. **Cost Optimization** (High Priority at Scale)
- **Intelligent Model Routing**:
  - Simple queries â†’ GPT-3.5-turbo (5x cheaper)
  - Complex queries â†’ GPT-4
  - Classification model to determine query complexity
- **Token Optimization**:
  - Compress context using summarization before passing to LLM
  - Truncate chunks intelligently (keep most relevant sentences)
  - Use smaller max_tokens when possible
- **Embedding Caching**: Cache embeddings for static content (save 90% of embedding costs)
- **Batch Processing**: Batch multiple embeddings in single API call (up to 2048 inputs/request)
- **Provider Diversity**: Support multiple LLM providers (Anthropic, local models) for cost comparison
- **Usage Analytics**: Track cost per query, identify expensive patterns

### 9. **Testing & Quality Assurance** (High Priority)
- **Comprehensive Test Suite**:
  - Unit tests: 80%+ coverage on business logic
  - Integration tests: Real API calls in CI with test accounts
  - E2E tests: Full user journey validation
  - Contract tests: Verify API contracts remain stable
- **Load Testing**: k6 or Locust scripts to test 100+ concurrent users
- **Chaos Engineering**: Test failure scenarios (API outages, network issues)
- **Regression Testing**: Track answer quality over time with golden dataset
- **A/B Testing Framework**: Compare different chunking strategies, retrieval params, prompts

### 10. **DevOps & Infrastructure** (Medium Priority)
- **CI/CD Pipeline**:
  - **GitHub Actions** or **Azure DevOps Pipelines**: lint, test, build, deploy on merge to main
  - Azure DevOps: YAML pipelines with multi-stage deployments (build â†’ test â†’ deploy)
  - Automated Docker image builds and registry pushes (ACR, ECR, Docker Hub)
  - Blue-green or canary deployments for zero-downtime updates
  - Automated rollback on health check failures
- **Infrastructure as Code**: Terraform for cloud resources (VPC, load balancers, etc.)
- **Multi-environment Setup**: dev, staging, production with environment parity
- **Backup & Disaster Recovery**: 
  - Vector database backups (daily snapshots)
  - Config backup in version control
  - Disaster recovery runbook
- **Auto-scaling**: Configure HPA based on request latency, not just CPU
- **CDN Integration**: CloudFront or Cloudflare for API caching at edge

### Priority Matrix

**Implement First (Weeks 1-2):**
- RAG quality (hybrid search, reranking)
- Observability basics (logging, metrics)
- Security fundamentals (auth, rate limiting)

**Implement Next (Weeks 3-4):**
- Performance optimizations (caching, async)
- Architecture refactoring (package structure)
- Testing expansion (integration, load tests)

**Long-term (Months 2-3):**
- Advanced features (conversation memory, feedback loops)
- Cost optimization at scale
- Full DevOps automation

## ğŸ”§ Troubleshooting

### Common Issues

#### 1. **Pinecone Connection Errors**
```
pinecone.core.client.exceptions.UnauthorizedException: (401)
Reason: Unauthorized
```
**Cause:** Invalid API key or incorrect environment/index configuration

**Solutions:**
- Verify `PINECONE_API_KEY` in `.env` matches your Pinecone dashboard
- Check `PINECONE_INDEX_NAME` exists in your Pinecone project (default: `typeform-help-center`)
- Verify index is in correct region (check Pinecone console)
- For serverless: ensure `PINECONE_ENVIRONMENT` is not set (legacy parameter)

**Diagnostics:**
```bash
# Test Pinecone connection
uv run python -c "from pinecone import Pinecone; pc = Pinecone(api_key='YOUR_KEY'); print(pc.list_indexes())"
```

#### 2. **OpenAI API Errors**
```
openai.AuthenticationError: Incorrect API key provided
```
**Cause:** Invalid API key or shell environment variable override

**Solutions:**
- Verify `OPENAI_API_KEY` is correct in `.env` file
- **Critical:** If `OPENAI_API_KEY` exists as shell variable, it overrides `.env`
  ```bash
  echo $OPENAI_API_KEY  # Check if set
  unset OPENAI_API_KEY  # Remove shell variable
  ```
- For Docker: Remove shell variable and restart containers
  ```bash
  unset OPENAI_API_KEY
  docker-compose down && docker-compose up -d
  ```
- Check OpenAI account has active credits/billing
- Verify API key permissions (needs embeddings + chat completions)

**Rate Limit Errors:**
```
openai.RateLimitError: Rate limit exceeded
```
- Tier 1 limits: 500 RPM, 200,000 TPM
- Solution: Implement exponential backoff or upgrade tier

#### 3. **uv Package Manager Issues**
```
error: Failed to download distributions
```
**Cause:** Network issues, package conflicts, or version constraints

**Solutions:**
- Clear cache: `uv cache clean`
- Reinstall dependencies: `rm -rf .venv && uv sync`
- Check Python version: `uv run python --version` (requires 3.11+)
- Network issues: `uv sync --index-url https://pypi.org/simple`

**Module Import Errors:**
```
ModuleNotFoundError: No module named 'xxx'
```
- Ensure virtual environment is activated or use `uv run`
- Reinstall: `uv sync --reinstall-package <package-name>`

#### 4. **Docker Container Issues**

**Container Exits Immediately:**
```bash
# Check logs for error details
docker-compose logs api
```
**Common causes:**
- Missing `.env` file â†’ create from `.env.example`
- Port 8000 already in use â†’ change `API_PORT` in `.env`
- Invalid environment variables â†’ check syntax, no quotes around values

**Health Check Failures:**
```
Health check failed: Get "http://localhost:8000/health": connection refused
```
**Solutions:**
- Check container is running: `docker-compose ps`
- Inspect logs: `docker-compose logs -f api`
- Verify port mapping: `docker port rag_api`
- Test from inside container: `docker exec rag_api curl http://localhost:8000/health`

**Network Issues:**
```
curl: (7) Failed to connect to 0.0.0.0 port 8000
```
**Solutions:**
- Use `http://127.0.0.1:8000` instead of `0.0.0.0`
- Check firewall rules
- Verify container ports: `docker-compose ps`

#### 5. **Database Not Populated**
```json
{
  "total_vectors": 0,
  "dimension": 1536,
  "index_fullness": 0.0
}
```
**Cause:** `init_db.py` not run or failed during indexing

**Solutions:**
1. Check if init completed successfully:
   ```bash
   curl http://localhost:8000/index/stats
   ```
2. Re-run initialization:
   ```bash
   uv run python init_db.py
   ```
3. Check logs for errors during indexing
4. Verify `sample_articles.py` contains articles

**Empty or Poor Quality Answers:**
- Check retrieval is working: Look at `context` in `/query` response
- Adjust `TOP_K_RESULTS` (try 5-10)
- Verify embeddings model matches index dimension (1536)

#### 6. **Performance Issues**

**Slow Response Times (>5 seconds):**
**Diagnostics:**
```bash
# Test with timing
time curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question":"test"}' 
```
**Solutions:**
- Reduce `TOP_K_RESULTS` (default 5 is optimal)
- Use GPT-3.5-turbo instead of GPT-4:
  ```bash
  export LLM_MODEL=gpt-3.5-turbo
  ```
- Check network latency to OpenAI/Pinecone APIs
- Enable caching (see Future Improvements)

**Out of Memory (OOM Killed):**
```
Error: Container killed due to memory usage
```
**Solutions:**
- Increase Docker memory: Docker Desktop â†’ Settings â†’ Resources â†’ 4GB+
- Kubernetes: Increase `memory` limits in deployment.yaml
- Reduce batch size in `init_db.py` (process fewer articles)
- Reduce `CHUNK_SIZE` to create smaller embeddings batches

#### 7. **Kubernetes Deployment Issues**

**Pods CrashLooping:**
```bash
kubectl logs <pod-name> -n rag-system
kubectl describe pod <pod-name> -n rag-system
```
**Common causes:**
- Missing secrets: `kubectl get secret openai-api-key -n rag-system`
- ConfigMap errors: Verify `kubectl get configmap rag-config -n rag-system`
- Resource constraints: Check `kubectl top pod -n rag-system`

**ImagePullBackOff:**
```
Failed to pull image "your-registry/rag-api:latest"
```
**Solutions:**
- Verify image exists: `docker images | grep rag-api`
- Push to registry: `docker push <your-registry>/rag-api:latest`
- Update `image:` in `deployment.yaml`
- Check imagePullSecrets if using private registry

**Service Not Reachable:**
```bash
# Test from within cluster
kubectl run -it --rm debug --image=curlimages/curl --restart=Never -- \
  curl http://rag-api-service.rag-system.svc.cluster.local:8000/health
```

#### 8. **API Response Errors**

**503 Service Unavailable:**
```json
{
  "detail": "Vector database is not ready"
}
```
**Cause:** Pinecone index not initialized or connection failed

**Solutions:**
- Check `/health` endpoint status
- Verify Pinecone credentials
- Check network connectivity to Pinecone

**401 Unauthorized (Admin Endpoints):**
```json
{
  "detail": "Invalid or missing admin token"
}
```
**Solutions:**
- Include header: `-H "X-Admin-Token: your-secret-token"`
- Verify `ADMIN_TOKEN` in `.env`
- Default token: `dev-admin-token-change-in-production`

**422 Validation Error:**
```json
{
  "detail": [
    {
      "loc": ["body", "question"],
      "msg": "field required"
    }
  ]
}
```
**Cause:** Missing or invalid request parameters

**Solutions:**
- Check API documentation for required fields
- Verify Content-Type header: `application/json`
- Validate JSON syntax

### Debug Mode

Enable detailed logging for troubleshooting:

```bash
# Local development
export LOG_LEVEL=DEBUG
uv run python main.py
```

```bash
# Docker
docker-compose down
LOG_LEVEL=DEBUG docker-compose up
```

**What DEBUG logging shows:**
- Request/response payloads
- Embeddings generation details
- Vector search queries and scores
- LLM prompts and completions
- Timing for each pipeline stage

### Health Check Diagnostics

```bash
# Check all health indicators
curl http://localhost:8000/health | python3 -m json.tool

# Expected healthy response:
{
  "status": "healthy",
  "timestamp": "2026-01-18T10:30:00.123456",
  "environment": "development",
  "database": "connected",
  "vector_count": 42
}
```

### Getting Help

If issues persist:

1. **Check logs** with DEBUG level enabled
2. **Verify environment variables**: `uv run python -c "from config import settings; print(settings.model_dump())"`
3. **Test components individually**:
   - OpenAI: `uv run python -c "from openai import OpenAI; OpenAI().models.list()"`
   - Pinecone: See diagnostics in Issue #1
4. **Review error traces** for specific line numbers and stack traces
5. **Check resource usage**: `docker stats` or `kubectl top pods`

## ğŸ“„ License

This project is created for a technical assessment for Typeform.

## ğŸ™‹â€â™€ï¸ Support

For questions or issues:
1. Check the troubleshooting section
2. Review the logs for error messages
3. Ensure all prerequisites are met
4. Verify environment variables are set correctly
